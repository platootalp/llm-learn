import os
import logging
from typing import Dict, Any, Optional
from langchain_openai import ChatOpenAI
from langchain_core.language_models.chat_models import BaseChatModel

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load environment variables once (optional)
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    logger.warning("python-dotenv module not found, skipping .env file loading")
except Exception as e:
    logger.warning(f"Failed to load .env file: {e}")


class LLMFactory:
    """
    å¤§è¯­è¨€æ¨¡å‹å·¥å‚ç±»ï¼Œç”¨äºåˆ›å»ºå’Œç®¡ç†ä¸åŒç±»å‹çš„LLMå®ä¾‹
    å®ç°å•ä¾‹æ¨¡å¼ï¼Œç¡®ä¿æ¯ä¸ªæ¨¡å‹åªåˆ›å»ºä¸€æ¬¡
    """
    
    # æ¨¡å‹é…ç½®æ˜ å°„ - å¢å¼ºæ”¯æŒæ›´å¤šæ¨¡å‹å’Œé…ç½®é€‰é¡¹
    MODEL_CONFIGS = {
        "qwen": {
            "model_name": "qwen-turbo",
            "api_base_env": "DASHSCOPE_API_URL",
            "api_key_env": "DASHSCOPE_API_KEY",
            "default_api_base": "https://dashscope.aliyuncs.com/compatible-mode/v1",
            "model_class": ChatOpenAI,
            "default_params": {
                "temperature": 0.7,
                "max_tokens": 2048,
                "response_format": {"type": "text"}
            }
        },
        "mimo": {
            "model_name": "mimo-v2-flash",
            "api_base_env": "MIMO_API_URL",
            "api_key_env": "MIMO_API_KEY",
            "default_api_base": "https://api.mimo.ai/v1",
            "model_class": ChatOpenAI,
            "default_params": {
                "temperature": 0.6,
                "max_tokens": 4096,
                "response_format": {"type": "text"}
            }
        },
        "qwen-long": {
            "model_name": "qwen-max-longcontext",
            "api_base_env": "DASHSCOPE_API_URL",
            "api_key_env": "DASHSCOPE_API_KEY",
            "default_api_base": "https://dashscope.aliyuncs.com/compatible-mode/v1",
            "model_class": ChatOpenAI,
            "default_params": {
                "temperature": 0.5,
                "max_tokens": 8192,
                "response_format": {"type": "text"}
            }
        }
    }
    
    def __init__(self):
        self._model_instances: Dict[str, BaseChatModel] = {}
    
    def get_model(self, model_name: str, **kwargs) -> BaseChatModel:
        """
        è·å–æŒ‡å®šåç§°çš„æ¨¡å‹å®ä¾‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            **kwargs: é¢å¤–çš„æ¨¡å‹é…ç½®å‚æ•°
            
        Returns:
            BaseChatModel: æ¨¡å‹å®ä¾‹
            
        Raises:
            ValueError: å½“æ¨¡å‹åç§°ä¸æ”¯æŒæˆ–å‚æ•°æ— æ•ˆæ—¶
            KeyError: å½“ç¼ºå°‘å¿…è¦çš„ç¯å¢ƒå˜é‡æ—¶
            RuntimeError: å½“æ¨¡å‹å®ä¾‹åˆ›å»ºå¤±è´¥æ—¶
        """
        if not model_name:
            raise ValueError("æ¨¡å‹åç§°ä¸èƒ½ä¸ºç©º")
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
        if model_name in self._model_instances:
            logger.info(f"Returning cached model instance for: {model_name}")
            return self._model_instances[model_name]
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒ
        if model_name not in self.MODEL_CONFIGS:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹åç§°: {model_name}ã€‚æ”¯æŒçš„æ¨¡å‹: {list(self.MODEL_CONFIGS.keys())}")
        
        logger.info(f"Creating new model instance for: {model_name}")
        config = self.MODEL_CONFIGS[model_name]
        
        # è·å–ç¯å¢ƒå˜é‡
        try:
            api_base = os.environ.get(config["api_base_env"]) or config["default_api_base"]
            if not api_base:
                raise ValueError(f"API base URL is empty for model {model_name}")
            
            api_key = os.environ[config["api_key_env"]]
            if not api_key:
                raise ValueError(f"API key is empty for model {model_name}")
        except KeyError as e:
            raise KeyError(f"ç¼ºå°‘å¿…è¦çš„ç¯å¢ƒå˜é‡: {e}")
        except ValueError as e:
            raise ValueError(f"æ— æ•ˆçš„ç¯å¢ƒå˜é‡é…ç½®: {e}")
        
        # æ„å»ºæ¨¡å‹å‚æ•° - ä½¿ç”¨æ¨¡å‹ç‰¹å®šçš„é»˜è®¤å‚æ•°
        model_kwargs = {
            "model_name": config["model_name"],
            "openai_api_base": api_base,
            "openai_api_key": api_key,
        }
        
        # æ·»åŠ æ¨¡å‹ç‰¹å®šçš„é»˜è®¤å‚æ•°
        if "default_params" in config:
            model_kwargs.update(config["default_params"])
        
        # æ·»åŠ é€šç”¨é»˜è®¤å‚æ•°ï¼ˆå¦‚æœæ¨¡å‹é…ç½®ä¸­æ²¡æœ‰æŒ‡å®šï¼‰
        model_kwargs.setdefault("temperature", 0.7)
        model_kwargs.setdefault("max_tokens", 2048)
        
        # åˆå¹¶é¢å¤–çš„å‚æ•°ï¼Œè¦†ç›–é»˜è®¤å€¼
        model_kwargs.update(kwargs)
        
        logger.debug(f"Model instantiation parameters for {model_name}: " + 
                    f"{{'model_name': '{model_kwargs['model_name']}', " +
                    f"'openai_api_base': '{api_base.split('://')[0] + '://' + '***' + api_base.split('://')[1][-10:] if '://' in api_base else '***'}', " +
                    f"'temperature': {model_kwargs['temperature']}, " +
                    f"'max_tokens': {model_kwargs['max_tokens']}}}")
        
        # åˆ›å»ºå…·ä½“æ¨¡å‹
        try:
            # æ”¯æŒè‡ªå®šä¹‰æ¨¡å‹ç±»
            model_class = config.get("model_class", ChatOpenAI)
            model = model_class(**model_kwargs)
            
            # ç¼“å­˜æ¨¡å‹å®ä¾‹
            self._model_instances[model_name] = model
            logger.info(f"Successfully created model instance for: {model_name}")
            return model
        except TypeError as e:
            logger.error(f"Invalid parameters for model {model_name}: {e}")
            raise ValueError(f"æ— æ•ˆçš„æ¨¡å‹å‚æ•°é…ç½®: {e}") from e
        except Exception as e:
            logger.error(f"Failed to create model instance for {model_name}: {e}", exc_info=True)
            raise RuntimeError(f"æ¨¡å‹å®ä¾‹åˆ›å»ºå¤±è´¥: {str(e)}") from e
    
    def list_supported_models(self) -> list[str]:
        """
        è·å–æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
        
        Returns:
            list[str]: æ”¯æŒçš„æ¨¡å‹åç§°åˆ—è¡¨
        """
        return list(self.MODEL_CONFIGS.keys())
    
    def clear_cache(self) -> None:
        """
        æ¸…é™¤æ‰€æœ‰ç¼“å­˜çš„æ¨¡å‹å®ä¾‹
        """
        logger.info("Clearing all cached model instances")
        self._model_instances.clear()
    
    def get_qwen_model(self, **kwargs) -> ChatOpenAI:
        """
        ä¾¿æ·æ–¹æ³•ï¼šè·å– Qwen æ¨¡å‹å®ä¾‹
        
        Args:
            **kwargs: é¢å¤–çš„æ¨¡å‹é…ç½®å‚æ•°
            
        Returns:
            ChatOpenAI: Qwen æ¨¡å‹å®ä¾‹
        """
        return self.get_model("qwen", **kwargs)
    
    def get_mimo_model(self, **kwargs) -> ChatOpenAI:
        """
        ä¾¿æ·æ–¹æ³•ï¼šè·å– MIMO æ¨¡å‹å®ä¾‹
        
        Args:
            **kwargs: é¢å¤–çš„æ¨¡å‹é…ç½®å‚æ•°
            
        Returns:
            ChatOpenAI: MIMO æ¨¡å‹å®ä¾‹
        """
        return self.get_model("mimo", **kwargs)
    
    def update_model_config(self, model_name: str, **config_updates) -> None:
        """
        æ›´æ–°æŒ‡å®šæ¨¡å‹çš„é…ç½®
        
        Args:
            model_name: æ¨¡å‹åç§°
            **config_updates: è¦æ›´æ–°çš„é…ç½®å‚æ•°
            
        Raises:
            ValueError: å½“æ¨¡å‹åç§°ä¸æ”¯æŒæ—¶
        """
        if model_name not in self.MODEL_CONFIGS:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹åç§°: {model_name}ã€‚æ”¯æŒçš„æ¨¡å‹: {list(self.MODEL_CONFIGS.keys())}")
        
        self.MODEL_CONFIGS[model_name].update(config_updates)
        logger.info(f"Updated configuration for model {model_name}: {config_updates}")
        
        # å¦‚æœæ¨¡å‹å®ä¾‹å·²å­˜åœ¨ï¼Œæ¸…é™¤ç¼“å­˜ä»¥åº”ç”¨æ–°é…ç½®
        if model_name in self._model_instances:
            logger.info(f"Clearing cached instance for model {model_name} to apply new configuration")
            del self._model_instances[model_name]
    
    def validate_environment(self) -> Dict[str, bool]:
        """
        éªŒè¯æ‰€æœ‰æ”¯æŒæ¨¡å‹çš„ç¯å¢ƒå˜é‡æ˜¯å¦é…ç½®æ­£ç¡®
        
        Returns:
            Dict[str, bool]: æ¯ä¸ªæ¨¡å‹çš„ç¯å¢ƒå˜é‡éªŒè¯ç»“æœ
        """
        validation_results = {}
        
        for model_name, config in self.MODEL_CONFIGS.items():
            try:
                # æ£€æŸ¥APIå¯†é’¥æ˜¯å¦å­˜åœ¨
                api_key = os.environ[config["api_key_env"]]
                if not api_key:
                    raise ValueError(f"API key for {model_name} is empty")
                
                # API baseæ˜¯å¯é€‰çš„ï¼Œå› ä¸ºæœ‰é»˜è®¤å€¼
                validation_results[model_name] = True
            except (KeyError, ValueError) as e:
                logger.warning(f"Environment validation failed for {model_name}: {e}")
                validation_results[model_name] = False
        
        return validation_results


# åˆ›å»ºå·¥å‚å®ä¾‹ï¼ˆå•ä¾‹ä½¿ç”¨ï¼‰
llm_factory = LLMFactory()


if __name__ == '__main__':
    # æµ‹è¯•å·¥å‚åŠŸèƒ½
    try:
        # åˆ—å‡ºæ”¯æŒçš„æ¨¡å‹
        print(f"æ”¯æŒçš„æ¨¡å‹: {llm_factory.list_supported_models()}")
        
        # éªŒè¯ç¯å¢ƒé…ç½®
        print("\néªŒè¯ç¯å¢ƒé…ç½®...")
        env_validation = llm_factory.validate_environment()
        for model_name, is_valid in env_validation.items():
            status = "âœ… æœ‰æ•ˆ" if is_valid else "âŒ æ— æ•ˆ"
            print(f"{model_name}: {status}")
        
        # é€‰æ‹©ä¸€ä¸ªç¯å¢ƒæœ‰æ•ˆä¸”æ”¯æŒçš„æ¨¡å‹è¿›è¡Œæµ‹è¯•
        available_models = [model for model, valid in env_validation.items() if valid]
        
        if not available_models:
            print("\nâŒ æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ç¯å¢ƒé…ç½®ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡è®¾ç½®")
            exit(1)
        
        test_model = available_models[0]
        print(f"\né€‰æ‹©æµ‹è¯•æ¨¡å‹: {test_model}")
        
        # æµ‹è¯•è·å–æ¨¡å‹å®ä¾‹
        model = llm_factory.get_model(test_model, temperature=0.5)
        
        # æµ‹è¯•å†æ¬¡è·å–åŒä¸€æ¨¡å‹ï¼ˆåº”è¯¥è¿”å›ç¼“å­˜å®ä¾‹ï¼‰
        cached_model = llm_factory.get_model(test_model)
        print(f"\næ¨¡å‹ç¼“å­˜æµ‹è¯•: {model is cached_model} (åŒä¸€å®ä¾‹è¡¨ç¤ºç¼“å­˜ç”Ÿæ•ˆ)")
        
        # æµ‹è¯•æ¨¡å‹ç”Ÿæˆ
        print("\næµ‹è¯•æ¨¡å‹ç”Ÿæˆ...")
        prompt = "å¤©ç©ºæ˜¯ä»€ä¹ˆé¢œè‰²çš„ï¼Ÿ"
        print(f"æç¤ºè¯: {prompt}")
        print("å›ç­”: ", end="", flush=True)
        
        chunks = []
        for chunk in model.stream(prompt):
            chunks.append(chunk)
            print(chunk.content, end="", flush=True)
            
        print("\n\nç”Ÿæˆå®Œæˆï¼")
        
        # æµ‹è¯•é…ç½®æ›´æ–°
        print("\næµ‹è¯•é…ç½®æ›´æ–°åŠŸèƒ½...")
        if test_model in llm_factory.list_supported_models():
            # æ›´æ–°å…¶ä»–é…ç½®å‚æ•°ï¼Œè€Œä¸æ˜¯model_name
            llm_factory.update_model_config(test_model, temperature=0.8)
            print(f"âœ… æˆåŠŸæ›´æ–°æ¨¡å‹ {test_model} çš„é…ç½®")
        
        # æµ‹è¯•æ¸…é™¤ç¼“å­˜
        print("\næµ‹è¯•æ¸…é™¤ç¼“å­˜åŠŸèƒ½...")
        llm_factory.clear_cache()
        print("âœ… æˆåŠŸæ¸…é™¤æ‰€æœ‰æ¨¡å‹ç¼“å­˜")
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}", exc_info=True)
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
