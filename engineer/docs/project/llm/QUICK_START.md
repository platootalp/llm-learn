# LLM æ ¸å¿ƒæ¨¡å—å¿«é€Ÿä¸Šæ‰‹æŒ‡å—

## ğŸš€ 5åˆ†é’Ÿå¿«é€Ÿå¼€å§‹

### 1. æœ€ç®€å•çš„ä½¿ç”¨æ–¹å¼

```python
from src.core import init_chat_model

# åˆå§‹åŒ–æ¨¡å‹ï¼ˆè‡ªåŠ¨æ¨æ–­ç±»å‹ï¼‰
model = init_chat_model("gpt-4")

# è°ƒç”¨æ¨¡å‹
response = model.invoke("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±")
print(response.content)
```

### 2. ä½¿ç”¨ç±»å‹ç‰¹å®šå·¥å‚

```python
from src.core import ChatOpenAI, ChatAnthropic, ChatOllama

# OpenAI
model = ChatOpenAI(model_name="gpt-4")

# Anthropic
model = ChatAnthropic(model_name="claude-3-opus-20240229")

# Ollama (æœ¬åœ°)
model = ChatOllama(model_name="llama2")

# ç»Ÿä¸€çš„æ¥å£
response = model.invoke("ä½ å¥½")
```

### 3. æµå¼è¾“å‡º

```python
from src.core import init_chat_model

model = init_chat_model("gpt-4")

# æµå¼è¾“å‡º
for chunk in model.stream("è®²ä¸ªç¬‘è¯"):
    print(chunk, end='', flush=True)
```

## ğŸ“š æ ¸å¿ƒæ¦‚å¿µ

### æ¶ˆæ¯ç±»å‹

```python
from src.core import HumanMessage, AIMessage, SystemMessage

# ç³»ç»Ÿæ¶ˆæ¯ï¼šè®¾ç½®åŠ©æ‰‹è¡Œä¸º
system_msg = SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹")

# äººç±»æ¶ˆæ¯ï¼šç”¨æˆ·è¾“å…¥
human_msg = HumanMessage(content="ä½ å¥½")

# AI æ¶ˆæ¯ï¼šåŠ©æ‰‹å›å¤
ai_msg = AIMessage(content="ä½ å¥½ï¼æˆ‘èƒ½å¸®ä½ ä»€ä¹ˆï¼Ÿ")
```

### å¤šè½®å¯¹è¯

```python
from src.core import init_chat_model, HumanMessage, SystemMessage

model = init_chat_model("gpt-4")

messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªPythonä¸“å®¶"),
    HumanMessage(content="ä»€ä¹ˆæ˜¯åˆ—è¡¨æ¨å¯¼å¼ï¼Ÿ"),
]

response = model.invoke(messages)
print(response.content)

# ç»§ç»­å¯¹è¯
messages.append(AIMessage(content=response.content))
messages.append(HumanMessage(content="è¯·ç»™ä¸ªä¾‹å­"))

response = model.invoke(messages)
print(response.content)
```

## ğŸ¯ å¸¸ç”¨åœºæ™¯

### åœºæ™¯ 1: ç®€å•é—®ç­”

```python
from src.core import quick_invoke

# å¿«é€Ÿè°ƒç”¨
answer = quick_invoke(
    text="ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
    model="gpt-3.5-turbo"
)
print(answer)
```

### åœºæ™¯ 2: æ‰¹é‡å¤„ç†

```python
from src.core import init_chat_model

model = init_chat_model("gpt-3.5-turbo")

questions = [
    "ä»€ä¹ˆæ˜¯Pythonï¼Ÿ",
    "ä»€ä¹ˆæ˜¯JavaScriptï¼Ÿ",
    "ä»€ä¹ˆæ˜¯Javaï¼Ÿ"
]

# æ‰¹é‡è°ƒç”¨
answers = model.batch(questions)

for q, a in zip(questions, answers):
    print(f"Q: {q}")
    print(f"A: {a.content}\n")
```

### åœºæ™¯ 3: å¼‚æ­¥è°ƒç”¨

```python
import asyncio
from src.core import init_chat_model

async def main():
    model = init_chat_model("gpt-4")
    
    # å¼‚æ­¥è°ƒç”¨
    response = await model.ainvoke("ä½ å¥½")
    print(response.content)
    
    # å¼‚æ­¥æ‰¹é‡
    responses = await model.abatch([
        "é—®é¢˜1",
        "é—®é¢˜2",
        "é—®é¢˜3"
    ])
    
    for resp in responses:
        print(resp.content)

asyncio.run(main())
```

### åœºæ™¯ 4: é…ç½®å‚æ•°

```python
from src.core import ChatOpenAI

model = ChatOpenAI(
    model_name="gpt-4",
    temperature=0.7,        # æ§åˆ¶éšæœºæ€§
    max_tokens=2000,        # æœ€å¤§è¾“å‡ºé•¿åº¦
    api_key="sk-...",      # API å¯†é’¥
)

response = model.invoke("å†™ä¸€é¦–è¯—")
```

## ğŸ”§ é«˜çº§ç”¨æ³•

### ä½¿ç”¨é…ç½®å¯¹è±¡

```python
from src.core import ChatModelConfig, ModelType, create_chat_model

config = ChatModelConfig(
    model_name="gpt-4",
    model_type=ModelType.OPENAI,
    temperature=0.8,
    max_tokens=3000,
    api_key="sk-..."
)

model = create_chat_model(config)
```

### æ€§èƒ½ç›‘æ§

```python
from src.core import init_chat_model

model = init_chat_model("gpt-4")

# è°ƒç”¨æ¨¡å‹
response = model.invoke("ä½ å¥½")

# æŸ¥çœ‹æ€§èƒ½æŒ‡æ ‡
metrics = model.get_metrics()
print(f"å¹³å‡å»¶è¿Ÿ: {metrics.avg_latency:.2f}ç§’")
print(f"æˆåŠŸç‡: {metrics.success_rate * 100:.1f}%")
print(f"æ€»è°ƒç”¨æ¬¡æ•°: {metrics.total_calls}")
```

## ğŸ“– æ”¯æŒçš„æ¨¡å‹

### OpenAI

```python
from src.core import ChatOpenAI

model = ChatOpenAI(model_name="gpt-4")
model = ChatOpenAI(model_name="gpt-3.5-turbo")
```

### Anthropic

```python
from src.core import ChatAnthropic

model = ChatAnthropic(model_name="claude-3-opus-20240229")
model = ChatAnthropic(model_name="claude-3-sonnet-20240229")
```

### é€šä¹‰åƒé—®

```python
from src.core import init_chat_model

model = init_chat_model(
    model="qwen-turbo",
    model_provider="qwen"
)
```

### Ollama (æœ¬åœ°)

```python
from src.core import ChatOllama

model = ChatOllama(
    model_name="llama2",
    base_url="http://localhost:11434"
)
```

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. ä½¿ç”¨ç¯å¢ƒå˜é‡ç®¡ç†å¯†é’¥

```bash
# .env æ–‡ä»¶
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
DASHSCOPE_API_KEY=sk-...
```

```python
# è‡ªåŠ¨ä»ç¯å¢ƒå˜é‡è¯»å–
from src.core import ChatOpenAI

model = ChatOpenAI(model_name="gpt-4")  # è‡ªåŠ¨è¯»å– OPENAI_API_KEY
```

### 2. é”™è¯¯å¤„ç†

```python
from src.core import init_chat_model

model = init_chat_model("gpt-4")

try:
    response = model.invoke("ä½ å¥½")
    print(response.content)
except Exception as e:
    print(f"è°ƒç”¨å¤±è´¥: {e}")
```

### 3. æµå¼è¾“å‡ºä¼˜åŒ–ç”¨æˆ·ä½“éªŒ

```python
from src.core import init_chat_model

model = init_chat_model("gpt-4")

print("AI: ", end='', flush=True)
for chunk in model.stream("è¯·è¯¦ç»†ä»‹ç»äººå·¥æ™ºèƒ½"):
    print(chunk, end='', flush=True)
print()
```

## ğŸ†š æ–°æ—§æ¥å£å¯¹æ¯”

### æ—§æ¥å£ï¼ˆä»å¯ç”¨ï¼‰

```python
from src.core import create_model, Message

model = create_model("gpt-4")
response = model.chat([Message(role="user", content="ä½ å¥½")])
print(response.content)
```

### æ–°æ¥å£ï¼ˆæ¨èï¼‰

```python
from src.core import init_chat_model

model = init_chat_model("gpt-4")
response = model.invoke("ä½ å¥½")
print(response.content)
```

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜ 1: API å¯†é’¥é”™è¯¯

```python
# ç¡®ä¿è®¾ç½®äº†ç¯å¢ƒå˜é‡
import os
print(os.getenv("OPENAI_API_KEY"))

# æˆ–è€…æ˜¾å¼ä¼ é€’
model = ChatOpenAI(model_name="gpt-4", api_key="sk-...")
```

### é—®é¢˜ 2: æ¨¡å‹ä¸å­˜åœ¨

```python
# ä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹åç§°
model = ChatOpenAI(model_name="gpt-4")  # âœ…
model = ChatOpenAI(model_name="gpt4")   # âŒ é”™è¯¯
```

### é—®é¢˜ 3: è¶…æ—¶é”™è¯¯

```python
# å¢åŠ è¶…æ—¶æ—¶é—´
model = init_chat_model(
    model="gpt-4",
    timeout=120  # ç§’
)
```

## ğŸ“š æ›´å¤šèµ„æº

- [å®Œæ•´ API æ–‡æ¡£](./README.md)
- [é‡æ„æŒ‡å—](./REFACTORING.md)
- [æä¾›å•†é…ç½®](./PROVIDERS.md)

## ğŸ‰ å¼€å§‹ä½¿ç”¨

ç°åœ¨ä½ å·²ç»æŒæ¡äº†åŸºç¡€çŸ¥è¯†ï¼Œå¯ä»¥å¼€å§‹ä½¿ç”¨äº†ï¼

```python
from src.core import init_chat_model

# åˆå§‹åŒ–ä½ çš„ç¬¬ä¸€ä¸ªæ¨¡å‹
model = init_chat_model("gpt-4")

# å¼€å§‹å¯¹è¯
response = model.invoke("ä½ å¥½ï¼è®©æˆ‘ä»¬å¼€å§‹å§")
print(response.content)
```

ç¥ä½¿ç”¨æ„‰å¿«ï¼ ğŸš€
