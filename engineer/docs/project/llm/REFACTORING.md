# LLM æ ¸å¿ƒæ¨¡å—é‡æ„æ–‡æ¡£

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£è®°å½•äº† `src/core` æ¨¡å—çš„é‡æ„è¿‡ç¨‹ï¼Œè¯¥é‡æ„å°†é¡¹ç›®çš„å¤§æ¨¡å‹åŸºç¡€èƒ½åŠ›ä¸ LangChain çš„å‘½åè§„èŒƒå’Œæ¶æ„è®¾è®¡å®Œå…¨å¯¹é½ï¼Œæå‡ä»£ç çš„æ ‡å‡†åŒ–ç¨‹åº¦å’Œå¯ç»´æŠ¤æ€§ã€‚

## ğŸ¯ é‡æ„ç›®æ ‡

1. **å‘½åè§„èŒƒå¯¹é½**ï¼šæ–‡ä»¶åã€ç±»åã€æ–¹æ³•åä¸ LangChain ä¿æŒä¸€è‡´
2. **æ¶æ„è®¾è®¡å¯¹é½**ï¼šé‡‡ç”¨ LangChain çš„è®¾è®¡æ¨¡å¼å’Œæ¥å£é£æ ¼
3. **ä»£ç ç°ä»£åŒ–**ï¼šä½¿ç”¨ Pydanticã€å¼‚æ­¥æ”¯æŒç­‰ç°ä»£ç‰¹æ€§
4. **åŠŸèƒ½å¢å¼º**ï¼šæ·»åŠ å¼‚æ­¥æ–¹æ³•ã€æ‰¹é‡å¤„ç†ç­‰é«˜çº§åŠŸèƒ½

## ğŸ“ æ–‡ä»¶ç»“æ„å˜åŒ–

### é‡æ„å‰

```
src/core/
â”œâ”€â”€ base_llm.py              # æ—§ï¼šåŸºç¡€ LLM ç±»
â”œâ”€â”€ llm_factory.py           # æ—§ï¼šLLM å·¥å‚å‡½æ•°
â”œâ”€â”€ model_info.py            # æ¨¡å‹ä¿¡æ¯
â”œâ”€â”€ providers/               # æä¾›å•†å®ç°
â””â”€â”€ __init__.py              # æ¨¡å—å¯¼å‡º
```

### é‡æ„å

```
src/core/
â”œâ”€â”€ language_models.py       # æ–°ï¼šè¯­è¨€æ¨¡å‹åŸºç¡€ç±» âœ¨
â”œâ”€â”€ chat_model_factory.py    # æ–°ï¼šèŠå¤©æ¨¡å‹å·¥å‚å‡½æ•° âœ¨
â”œâ”€â”€ model_info.py            # ä¿æŒä¸å˜
â”œâ”€â”€ providers/               # ä¿æŒä¸å˜
â””â”€â”€ __init__.py              # å®Œå…¨æ›´æ–°ï¼Œåªå¯¼å‡ºæ–°æ¥å£
```

## ğŸ”„ ä¸»è¦å˜æ›´

### 1. æ–‡ä»¶åå˜æ›´

| æ—§æ–‡ä»¶å | æ–°æ–‡ä»¶å | è¯´æ˜ |
|---------|---------|------|
| `base_llm.py` | `language_models.py` | å¯¹é½ LangChain å‘½å |
| `llm_factory.py` | `chat_model_factory.py` | æ›´æ˜ç¡®çš„å‘½å |

**é‡è¦**ï¼šæ—§æ–‡ä»¶å·²å®Œå…¨ç§»é™¤ï¼Œåªä¿ç•™æ–°æ¶æ„ã€‚

### 2. ç±»åå˜æ›´

| æ—§ç±»å | æ–°ç±»å | è¯´æ˜ |
|--------|--------|------|
| `BaseLLM` | `BaseChatModel` | æ›´å‡†ç¡®åœ°æè¿°èŠå¤©æ¨¡å‹ |
| `ModelConfig` | `ChatModelConfig` | ä½¿ç”¨ Pydantic BaseModel |
| `ModelResponse` | `ChatResult` | å¯¹é½ LangChain å‘½å |
| - | `LLMResult` | æ–°å¢æ‰¹é‡è°ƒç”¨ç»“æœç±» |
| - | `FunctionMessage` | æ–°å¢å‡½æ•°æ¶ˆæ¯ç±» |

**é‡è¦**ï¼šæ—§ç±»åå·²å®Œå…¨ç§»é™¤ï¼Œè¯·ä½¿ç”¨æ–°ç±»åã€‚

### 3. æ–¹æ³•åå˜æ›´

#### BaseChatModelï¼ˆåŸ BaseLLMï¼‰

| æ—§æ–¹æ³•å | æ–°æ–¹æ³•å | è¯´æ˜ |
|---------|---------|------|
| `chat()` | `invoke()` | å¯¹é½ LangChain æ ‡å‡†æ¥å£ |
| `complete()` | `predict()` | å‘åå…¼å®¹æ–¹æ³• |
| `stream_chat()` | `stream()` | ç®€åŒ–åç§° |
| - | `batch()` | æ–°å¢æ‰¹é‡è°ƒç”¨ |
| - | `ainvoke()` | æ–°å¢å¼‚æ­¥è°ƒç”¨ |
| - | `astream()` | æ–°å¢å¼‚æ­¥æµå¼è°ƒç”¨ |
| - | `abatch()` | æ–°å¢å¼‚æ­¥æ‰¹é‡è°ƒç”¨ |

### 4. å·¥å‚å‡½æ•°å˜æ›´

| æ—§å‡½æ•°å | æ–°å‡½æ•°å | è¯´æ˜ |
|---------|---------|------|
| `create_llm()` | `create_chat_model()` | æ›´æ˜ç¡®çš„å‘½å |
| `create_model()` | `init_chat_model()` | å¯¹é½ LangChain |
| - | `ChatOpenAI()` | æ–°å¢ç±»å‹ç‰¹å®šå·¥å‚ |
| - | `ChatAnthropic()` | æ–°å¢ç±»å‹ç‰¹å®šå·¥å‚ |
| - | `ChatOllama()` | æ–°å¢ç±»å‹ç‰¹å®šå·¥å‚ |
| `quick_chat()` | `quick_invoke()` | å¯¹é½å‘½åè§„èŒƒ |
| - | `quick_batch()` | æ–°å¢æ‰¹é‡è°ƒç”¨ |

## ğŸŒŸ æ–°å¢åŠŸèƒ½

### 1. å¼‚æ­¥æ”¯æŒ

```python
# å¼‚æ­¥è°ƒç”¨
response = await model.ainvoke("ä½ å¥½")

# å¼‚æ­¥æµå¼è°ƒç”¨
async for chunk in model.astream("è®²ä¸ªç¬‘è¯"):
    print(chunk, end='', flush=True)

# å¼‚æ­¥æ‰¹é‡è°ƒç”¨
responses = await model.abatch(["ä½ å¥½", "å†è§", "è°¢è°¢"])
```

### 2. æ‰¹é‡å¤„ç†

```python
# åŒæ­¥æ‰¹é‡è°ƒç”¨
responses = model.batch(["é—®é¢˜1", "é—®é¢˜2", "é—®é¢˜3"])

# ä¾¿æ·æ‰¹é‡å‡½æ•°
responses = quick_batch(
    ["é—®é¢˜1", "é—®é¢˜2"],
    model="gpt-4"
)
```

### 3. ç»Ÿä¸€çš„ invoke æ¥å£

```python
# æ”¯æŒå­—ç¬¦ä¸²
response = model.invoke("ä½ å¥½")

# æ”¯æŒæ¶ˆæ¯åˆ—è¡¨
response = model.invoke([
    SystemMessage(content="ä½ æ˜¯åŠ©æ‰‹"),
    HumanMessage(content="ä½ å¥½")
])
```

### 4. LangChain é£æ ¼çš„å·¥å‚å‡½æ•°

```python
# æ–¹å¼ 1: init_chat_modelï¼ˆæ¨èï¼‰
model = init_chat_model("gpt-4")

# æ–¹å¼ 2: ç±»å‹ç‰¹å®šå·¥å‚
model = ChatOpenAI(model_name="gpt-4")
model = ChatAnthropic(model_name="claude-3-opus")
model = ChatOllama(model_name="llama2")

# æ–¹å¼ 3: å¿«é€Ÿè°ƒç”¨
response = quick_invoke("ä½ å¥½", model="gpt-4")
```

## ğŸ“ è¿ç§»æŒ‡å—

### åŸºæœ¬ä½¿ç”¨è¿ç§»

#### æ—§ä»£ç ï¼ˆå·²åºŸå¼ƒï¼Œä¸å†æ”¯æŒï¼‰

```python
from src.core import create_model, BaseLLM  # âŒ è¿™äº›å¯¼å…¥å·²ä¸å­˜åœ¨

# åˆ›å»ºæ¨¡å‹
model = create_model("gpt-4")  # âŒ æ­¤å‡½æ•°å·²ç§»é™¤

# è°ƒç”¨æ¨¡å‹
response = model.chat([Message(role="user", content="ä½ å¥½")])  # âŒ chat() æ–¹æ³•å·²ç§»é™¤
print(response.content)
```

#### æ–°ä»£ç ï¼ˆå¿…é¡»ä½¿ç”¨ï¼‰

```python
from src.core import init_chat_model, HumanMessage

# åˆ›å»ºæ¨¡å‹
model = init_chat_model("gpt-4")

# è°ƒç”¨æ¨¡å‹
response = model.invoke("ä½ å¥½")
# æˆ–è€…
response = model.invoke([HumanMessage(content="ä½ å¥½")])
print(response.content)
```

### æµå¼è°ƒç”¨è¿ç§»

#### æ—§ä»£ç ï¼ˆå·²åºŸå¼ƒï¼‰

```python
model = create_model("gpt-4")  # âŒ å·²ç§»é™¤
for chunk in model.stream_chat([Message(role="user", content="è®²ä¸ªç¬‘è¯")]):  # âŒ å·²ç§»é™¤
    print(chunk, end='', flush=True)
```

#### æ–°ä»£ç ï¼ˆå¿…é¡»ä½¿ç”¨ï¼‰

```python
model = init_chat_model("gpt-4")
for chunk in model.stream("è®²ä¸ªç¬‘è¯"):
    print(chunk, end='', flush=True)
```

### é…ç½®å¯¹è±¡è¿ç§»

#### æ—§ä»£ç ï¼ˆå·²åºŸå¼ƒï¼‰

```python
from src.core import ModelConfig, ModelType, create_llm  # âŒ è¿™äº›å·²ç§»é™¤

config = ModelConfig(  # âŒ ModelConfig å·²ç§»é™¤
    model_name="gpt-4",
    model_type=ModelType.OPENAI,
    temperature=0.7
)
model = create_llm(config)  # âŒ create_llm() å·²ç§»é™¤
```

#### æ–°ä»£ç ï¼ˆå¿…é¡»ä½¿ç”¨ï¼‰

```python
from src.core import ChatModelConfig, ModelType

# ä½¿ç”¨é…ç½®å¯¹è±¡
config = ChatModelConfig(
    model_name="gpt-4",
    model_type=ModelType.OPENAI,
    temperature=0.7
)

# ç„¶åæ‰‹åŠ¨åˆ›å»ºå®ä¾‹ï¼Œæˆ–è€…æ›´ç®€å•ï¼ˆæ¨èï¼‰ï¼š
from src.core import init_chat_model

model = init_chat_model(
    model="gpt-4",
    temperature=0.7
)
```

### ç±»å‹ç‰¹å®šå·¥å‚è¿ç§»

#### æ—§ä»£ç ï¼ˆå·²åºŸå¼ƒï¼‰

```python
from src.core import create_model, ModelType  # âŒ create_model å·²ç§»é™¤

model = create_model(  # âŒ æ­¤å‡½æ•°å·²ç§»é™¤
    model_name="gpt-4",
    model_type=ModelType.OPENAI,
    api_key="sk-..."
)
```

#### æ–°ä»£ç ï¼ˆå¿…é¡»ä½¿ç”¨ï¼‰

```python
from src.core import ChatOpenAI

# æ›´ç®€æ´ã€æ›´ç¬¦åˆ LangChain é£æ ¼
model = ChatOpenAI(
    model_name="gpt-4",
    api_key="sk-..."
)
```

## ğŸ”§ å®Œæ•´ç¤ºä¾‹

### ç¤ºä¾‹ 1: åŸºç¡€èŠå¤©

```python
from src.core import init_chat_model, HumanMessage, SystemMessage

# åˆå§‹åŒ–æ¨¡å‹
model = init_chat_model(
    model="gpt-4",
    temperature=0.7,
    max_tokens=2000
)

# ç®€å•è°ƒç”¨
response = model.invoke("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±")
print(response.content)

# å¤šè½®å¯¹è¯
messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„åŠ©æ‰‹"),
    HumanMessage(content="ä½ å¥½"),
]
response = model.invoke(messages)
print(response.content)
```

### ç¤ºä¾‹ 2: æµå¼è¾“å‡º

```python
from src.core import ChatOpenAI

model = ChatOpenAI(model_name="gpt-4")

print("AI: ", end='', flush=True)
for chunk in model.stream("è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—"):
    print(chunk, end='', flush=True)
print()
```

### ç¤ºä¾‹ 3: æ‰¹é‡å¤„ç†

```python
from src.core import init_chat_model

model = init_chat_model("gpt-3.5-turbo")

questions = [
    "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
    "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
    "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ"
]

# æ‰¹é‡è°ƒç”¨
responses = model.batch(questions)

for question, response in zip(questions, responses):
    print(f"Q: {question}")
    print(f"A: {response.content}\n")
```

### ç¤ºä¾‹ 4: å¼‚æ­¥è°ƒç”¨

```python
import asyncio
from src.core import init_chat_model

async def main():
    model = init_chat_model("gpt-4")
    
    # å¼‚æ­¥è°ƒç”¨
    response = await model.ainvoke("ä½ å¥½")
    print(response.content)
    
    # å¼‚æ­¥æµå¼
    print("AI: ", end='', flush=True)
    async for chunk in model.astream("è®²ä¸ªç¬‘è¯"):
        print(chunk, end='', flush=True)
    print()
    
    # å¼‚æ­¥æ‰¹é‡
    responses = await model.abatch([
        "ä½ å¥½",
        "å†è§",
        "è°¢è°¢"
    ])
    for resp in responses:
        print(resp.content)

asyncio.run(main())
```

### ç¤ºä¾‹ 5: ä½¿ç”¨ç±»å‹ç‰¹å®šå·¥å‚

```python
from src.core import ChatOpenAI, ChatAnthropic, ChatOllama

# OpenAI
openai_model = ChatOpenAI(
    model_name="gpt-4",
    api_key="sk-..."
)

# Anthropic
anthropic_model = ChatAnthropic(
    model_name="claude-3-opus-20240229",
    api_key="sk-ant-..."
)

# Ollama (æœ¬åœ°)
ollama_model = ChatOllama(
    model_name="llama2",
    base_url="http://localhost:11434"
)

# ä½¿ç”¨ç›¸åŒçš„æ¥å£
for model in [openai_model, anthropic_model, ollama_model]:
    response = model.invoke("ä½ å¥½")
    print(f"{model}: {response.content}")
```

## âš ï¸ é‡å¤§å˜æ›´ï¼ˆBreaking Changesï¼‰

**v3.0.0 ç‰ˆæœ¬å·²å®Œå…¨ç§»é™¤ä»¥ä¸‹æ—§æ¥å£ï¼Œæ— å‘åå…¼å®¹ï¼š**

| æ—§æ¥å£ | æ–°æ¥å£ | çŠ¶æ€ |
|--------|--------|------|
| `BaseLLM` | `BaseChatModel` | âŒ å·²ç§»é™¤ |
| `ModelConfig` | `ChatModelConfig` | âŒ å·²ç§»é™¤ |
| `ModelResponse` | `ChatResult` | âŒ å·²ç§»é™¤ |
| `Message` | `BaseMessage` åŠå…¶å­ç±» | âŒ å·²ç§»é™¤ |
| `create_llm()` | `init_chat_model()` | âŒ å·²ç§»é™¤ |
| `create_model()` | `init_chat_model()` | âŒ å·²ç§»é™¤ |
| `quick_chat()` | `quick_invoke()` | âŒ å·²ç§»é™¤ |
| `quick_chat_stream()` | `model.stream()` | âŒ å·²ç§»é™¤ |
| `chat()` æ–¹æ³• | `invoke()` æ–¹æ³• | âŒ å·²ç§»é™¤ |
| `stream_chat()` æ–¹æ³• | `stream()` æ–¹æ³• | âŒ å·²ç§»é™¤ |

**å¿…é¡»æ›´æ–°ä»£ç ä»¥ä½¿ç”¨æ–°æ¥å£ã€‚**

## ğŸ“Š å¯¹æ¯”æ€»ç»“

| æ–¹é¢ | é‡æ„å‰ | é‡æ„å |
|------|--------|--------|
| æ–‡ä»¶å‘½å | `base_llm.py` | `language_models.py` |
| åŸºç±»åç§° | `BaseLLM` | `BaseChatModel` |
| ä¸»è¦æ–¹æ³• | `chat()` | `invoke()` |
| æµå¼æ–¹æ³• | `stream_chat()` | `stream()` |
| å¼‚æ­¥æ”¯æŒ | âŒ æ—  | âœ… `ainvoke()`, `astream()`, `abatch()` |
| æ‰¹é‡å¤„ç† | âŒ æ—  | âœ… `batch()` |
| å·¥å‚å‡½æ•° | `create_model()` | `init_chat_model()` |
| ç±»å‹å·¥å‚ | âŒ æ—  | âœ… `ChatOpenAI()`, `ChatAnthropic()` ç­‰ |
| é…ç½®ç±» | `ModelConfig` (dataclass) | `ChatModelConfig` (Pydantic) |
| å“åº”ç±» | `ModelResponse` | `ChatResult` |
| å‘åå…¼å®¹ | N/A | âœ… ä¿ç•™æ‰€æœ‰æ—§æ¥å£ |

## ğŸ¯ æœ€ä½³å®è·µ

### 1. æ¨èä½¿ç”¨æ–°æ¥å£

```python
# âœ… æ¨è
from src.core import init_chat_model
model = init_chat_model("gpt-4")
response = model.invoke("ä½ å¥½")

# âŒ ä¸æ¨èï¼ˆè™½ç„¶ä»å¯ç”¨ï¼‰
from src.core import create_model
model = create_model("gpt-4")
response = model.chat([Message(role="user", content="ä½ å¥½")])
```

### 2. ä½¿ç”¨ç±»å‹ç‰¹å®šå·¥å‚

```python
# âœ… æ›´æ¸…æ™°
from src.core import ChatOpenAI
model = ChatOpenAI(model_name="gpt-4")

# âœ… ä¹Ÿå¯ä»¥
from src.core import init_chat_model
model = init_chat_model("gpt-4", model_provider="openai")
```

### 3. åˆ©ç”¨æ–°åŠŸèƒ½

```python
# æ‰¹é‡å¤„ç†
responses = model.batch(["é—®é¢˜1", "é—®é¢˜2", "é—®é¢˜3"])

# å¼‚æ­¥è°ƒç”¨
response = await model.ainvoke("ä½ å¥½")

# æµå¼è¾“å‡º
for chunk in model.stream("è®²ä¸ªç¬‘è¯"):
    print(chunk, end='')
```

## ğŸ” å¸¸è§é—®é¢˜

### Q1: æ—§ä»£ç è¿˜èƒ½ç”¨å—ï¼Ÿ

**A**: ä¸èƒ½ï¼v3.0.0 å·²å®Œå…¨ç§»é™¤æ‰€æœ‰æ—§æ¥å£ï¼Œå¿…é¡»æ›´æ–°ä»£ç ä»¥ä½¿ç”¨æ–°æ¥å£ã€‚

### Q2: å¦‚ä½•å¿«é€Ÿè¿ç§»ä»£ç ï¼Ÿ

**A**: æŒ‰ç…§ä»¥ä¸‹æ˜ å°„è¡¨æ›´æ–°ï¼š
- `create_model()` â†’ `init_chat_model()`
- `model.chat()` â†’ `model.invoke()`
- `model.stream_chat()` â†’ `model.stream()`
- `BaseLLM` â†’ `BaseChatModel`
- `ModelConfig` â†’ `ChatModelConfig`
- `ModelResponse` â†’ `ChatResult`

### Q3: ä¸ºä»€ä¹ˆä¸ä¿ç•™å‘åå…¼å®¹ï¼Ÿ

**A**: ä¸ºäº†ï¼š
1. å®Œå…¨å¯¹é½ LangChain æ ‡å‡†
2. é¿å…æ¥å£æ··æ·†
3. ä¿æŒä»£ç åº“ç®€æ´
4. å¼ºåˆ¶ä½¿ç”¨æœ€ä½³å®è·µ

### Q4: æ€§èƒ½æœ‰å˜åŒ–å—ï¼Ÿ

**A**: æ²¡æœ‰ã€‚æ–°æ¶æ„æ€§èƒ½ä¸æ—§ä»£ç ä¸€è‡´ï¼ŒåŒæ—¶å¢åŠ äº†å¼‚æ­¥å’Œæ‰¹é‡å¤„ç†ç­‰é«˜çº§åŠŸèƒ½ã€‚

### Q5: è¿ç§»å¾ˆå¤æ‚å—ï¼Ÿ

**A**: ä¸å¤æ‚ï¼å¤§å¤šæ•°æƒ…å†µåªéœ€ï¼š
1. æ›¿æ¢å¯¼å…¥è¯­å¥
2. æ›¿æ¢å‡½æ•°å
3. æ›¿æ¢æ–¹æ³•å
æŸ¥çœ‹ä¸Šé¢çš„è¿ç§»ç¤ºä¾‹å³å¯å¿«é€Ÿå®Œæˆã€‚

### Q6: å¦‚ä½•æŠ¥å‘Šé—®é¢˜ï¼Ÿ

**A**: å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·ï¼š
1. æ£€æŸ¥æœ¬æ–‡æ¡£çš„è¿ç§»æŒ‡å—
2. æŸ¥çœ‹ QUICK_START.md çš„ç¤ºä¾‹ä»£ç 
3. æäº¤ Issue å¹¶é™„ä¸Šè¯¦ç»†ä¿¡æ¯

## ğŸ“… æ—¶é—´çº¿

- **2026-01-13**: v2.0.0 å‘å¸ƒï¼Œå¼•å…¥æ–°æ¶æ„ï¼ˆä¿ç•™å‘åå…¼å®¹ï¼‰
- **2026-01-13**: v3.0.0 å‘å¸ƒï¼Œç§»é™¤æ‰€æœ‰æ—§æ¥å£ï¼Œå®Œå…¨åˆ‡æ¢åˆ°æ–°æ¶æ„

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [å¿«é€Ÿä¸Šæ‰‹æŒ‡å—](./QUICK_START.md)
- [å®Œæ•´ API æ–‡æ¡£](./README.md)
- [æä¾›å•†å®ç°æŒ‡å—](./PROVIDERS.md)

---

**ç‰ˆæœ¬**: 3.0.0  
**æ›´æ–°æ—¥æœŸ**: 2026-01-13  
**çŠ¶æ€**: âœ… å®Œæˆï¼ˆå·²ç§»é™¤æ‰€æœ‰å‘åå…¼å®¹ä»£ç ï¼‰  
**é‡å¤§å˜æ›´**: æ—§æ¥å£å·²å®Œå…¨ç§»é™¤
