from torch import nn
from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer, AutoConfig
import logging
import time
from huggingface_hub import HfFolder
import os

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_model_with_retry(model_name, max_retries=3, retry_delay=5):
    """å¸¦é‡è¯•æœºåˆ¶çš„æ¨¡å‹åŠ è½½å‡½æ•°"""
    for attempt in range(max_retries):
        try:
            logger.info(f"å°è¯•åŠ è½½æ¨¡å‹ {model_name} (ç¬¬ {attempt + 1} æ¬¡å°è¯•)")
            model = AutoModelForSequenceClassification.from_pretrained(model_name)
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            return model, tokenizer
        except Exception as e:
            logger.warning(f"åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}")
            if attempt < max_retries - 1:
                logger.info(f"ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                time.sleep(retry_delay)
            else:
                raise

def sentiment_analysis():
    try:
        model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
        logger.info("æ­£åœ¨åˆå§‹åŒ–æƒ…æ„Ÿåˆ†ææ¨¡å‹...")
        model, tokenizer = load_model_with_retry(model_name)
        classifier = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)
        
        # æµ‹è¯•æ–‡æœ¬
        test_texts = [
            "I love this movie!",
            "This is terrible.",
            "I'm not sure how I feel about this.",
            "Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ğŸ¤— Transformers."
        ]
        
        logger.info("å¼€å§‹æƒ…æ„Ÿåˆ†æ...")
        results = classifier(test_texts)
        
        # æ‰“å°ç»“æœ
        for text, result in zip(test_texts, results):
            print(f"æ–‡æœ¬: {text}")
            print(f"æƒ…æ„Ÿ: {result['label']}")
            print(f"ç½®ä¿¡åº¦: {result['score']:.4f}")
            print("-" * 50)
            
    except Exception as e:
        logger.error(f"å‘ç”Ÿé”™è¯¯: {str(e)}")
        raise

# def tokenizer_test():
#     tokenizer = AutoTokenizer.from_pretrained(model_name)
#     print(tokenizer("We are very happy to show you the ğŸ¤— Transformers library."))
#     print(tokenizer.tokenize("Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ğŸ¤— Transformers."))

def scaled_dot_product_attention():
    try:
        model_ckpt = "bert-base-uncased"
        logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹ {model_ckpt}...")
        model, tokenizer = load_model_with_retry(model_ckpt)

        text = "time flies like an arrow"
        inputs = tokenizer(text, return_tensors="pt", add_special_tokens=False)
        print(inputs.input_ids)

        config = AutoConfig.from_pretrained(model_ckpt)
        token_emb = nn.Embedding(config.vocab_size, config.hidden_size)
        print(token_emb)

        inputs_embeds = token_emb(inputs.input_ids)
        print(inputs_embeds.size())
    except Exception as e:
        logger.error(f"å‘ç”Ÿé”™è¯¯: {str(e)}")
        raise

if __name__ == "__main__":
    # è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œä½¿ç”¨æœ¬åœ°ç¼“å­˜
    os.environ["TRANSFORMERS_OFFLINE"] = "1"
    os.environ["HF_DATASETS_OFFLINE"] = "1"
    
    # è®¾ç½®æ¨¡å‹ç¼“å­˜ç›®å½•
    cache_dir = os.path.expanduser("~/.cache/huggingface")
    os.makedirs(cache_dir, exist_ok=True)
    os.environ["TRANSFORMERS_CACHE"] = cache_dir
    
    # sentiment_analysis()
    # tokenizer_test()
    scaled_dot_product_attention()